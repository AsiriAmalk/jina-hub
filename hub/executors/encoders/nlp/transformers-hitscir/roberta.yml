!TransformerRobertaEncoder
metas:
  batch_size: $BATCH_SIZE
  workspace: $TMP_WORKSPACE
  name: transformer_roberta_encoder
  py_modules: transformer_roberta.py

with:
  max_length: 128
  pooling_strategy: cls